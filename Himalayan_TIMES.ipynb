{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPAyddk90qsW",
        "outputId": "fb63e867-7d9f-44e8-ad5e-792b25a3a87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 requests\n",
        "pip install textblob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib3\n",
        "import re\n",
        "import requests\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Disable InsecureRequestWarning\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "def construct_article_link(title):\n",
        "    # Convert title to lowercase, replace spaces with dashes, and remove special characters\n",
        "    slug = re.sub(r'[^a-zA-Z0-9\\s]', '', title).lower().replace(' ', '-')\n",
        "    return f\"https://myrepublica.nagariknetwork.com/news/{slug}/\"\n",
        "\n",
        "def scrape_politics_articles(base_url, pages=3):\n",
        "    articles = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        response = requests.get(url, verify=False)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        for article in soup.find_all('div', class_='main-heading'):\n",
        "            title_elem = article.find('h2')\n",
        "            title = title_elem.text.strip() if title_elem else 'No title'\n",
        "            full_link = construct_article_link(title)\n",
        "\n",
        "            # Fetch the full article content\n",
        "            article_response = requests.get(full_link, verify=False)\n",
        "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
        "            content_elem = article_soup.find_all('p')\n",
        "            content = ' '.join([p.text.strip() for p in content_elem]) if content_elem else 'No content'\n",
        "\n",
        "            date_elem = article.find('p', class_='headline-time')\n",
        "            date = date_elem.text.strip() if date_elem else 'No date'\n",
        "\n",
        "            articles.append({\n",
        "                'Title': title,\n",
        "                'Link': full_link,\n",
        "                'Date': date,\n",
        "                'Content': content\n",
        "            })\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Base URL of myRepublica politics section\n",
        "base_url = 'https://myrepublica.nagariknetwork.com/category/politics'\n",
        "\n",
        "# Scrape political articles from the first 10 pages\n",
        "politics_data = scrape_politics_articles(base_url)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(politics_data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('myrepublica_politics_articles.csv', index=False)\n",
        "print(f\"Scraped {len(df)} articles and saved to 'myrepublica_politics_articles.csv'\")\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "  \"\"\"Analyzes the sentiment of a given text using TextBlob.\"\"\"\n",
        "  analysis = TextBlob(text)\n",
        "  if analysis.sentiment.polarity > 0:\n",
        "    return 'Positive'\n",
        "  elif analysis.sentiment.polarity < 0:\n",
        "    return 'Negative'\n",
        "  else:\n",
        "    return 'Neutral'\n",
        "\n",
        "def analyze_articles_for_person(df, person_name):\n",
        "  \"\"\"Searches for articles mentioning a specific person and analyzes their sentiment.\"\"\"\n",
        "\n",
        "  relevant_articles = []\n",
        "  for index, row in df.iterrows():\n",
        "    if person_name.lower() in row['Title'].lower():\n",
        "      relevant_articles.append(row)\n",
        "\n",
        "  sentiment_results = []\n",
        "  for article in relevant_articles:\n",
        "    sentiment = analyze_sentiment(article['Content'])\n",
        "    sentiment_results.append({\n",
        "        'Title': article['Title'],\n",
        "        'Link': article['Link'],\n",
        "        'Date': article['Date'],\n",
        "        'Sentiment': sentiment\n",
        "    })\n",
        "\n",
        "  return sentiment_results\n",
        "\n",
        "person_name = input(\"Enter the name of the person you want to analyze: \")\n",
        "sentiment_results = analyze_articles_for_person(df, person_name)\n",
        "\n",
        "if sentiment_results:\n",
        "  print(f\"\\nSentiment analysis for articles mentioning {person_name}:\")\n",
        "  for result in sentiment_results:\n",
        "    print(f\"Title: {result['Title']}\")\n",
        "    print(f\"Link: {result['Link']}\")\n",
        "    print(f\"Date: {result['Date']}\")\n",
        "    print(f\"Sentiment: {result['Sentiment']}\\n\")\n",
        "else:\n",
        "  print(f\"No articles found mentioning {person_name}.\")\n"
      ],
      "metadata": {
        "id": "TPWjwzaO1RCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc3e998-c74d-4633-8aec-749371de8d01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0  Home minister Ramesh Lekhak faces crucial test...   \n",
            "1          Koshi CM Karki secures vote of confidence   \n",
            "2         Arrest warrant issued against Durga Prasai   \n",
            "3  Education Act will be finalized soon: Gagan Thapa   \n",
            "4  Govtâ€™s common minimum program to be announced ...   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://myrepublica.nagariknetwork.com/news/ho...   \n",
            "1  https://myrepublica.nagariknetwork.com/news/ko...   \n",
            "2  https://myrepublica.nagariknetwork.com/news/ar...   \n",
            "3  https://myrepublica.nagariknetwork.com/news/ed...   \n",
            "4  https://myrepublica.nagariknetwork.com/news/go...   \n",
            "\n",
            "                                                Date  \\\n",
            "0  Published On: \\n                              ...   \n",
            "1  Published On: \\n                              ...   \n",
            "2  Published On: \\n                              ...   \n",
            "3  Published On: \\n                              ...   \n",
            "4  Published On: \\n                              ...   \n",
            "\n",
            "                                             Content  \n",
            "0  Thursday, 19 September 2024 11:35 AM Published...  \n",
            "1  Thursday, 19 September 2024 11:35 AM Published...  \n",
            "2  Thursday, 19 September 2024 11:35 AM Published...  \n",
            "3  Thursday, 19 September 2024 11:35 AM Published...  \n",
            "4  Thursday, 19 September 2024 11:35 AM 404 Oops,...  \n",
            "Scraped 30 articles and saved to 'myrepublica_politics_articles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVz8lJHmIxQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}